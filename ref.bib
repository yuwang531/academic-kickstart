
@inproceedings{abate_BoxInvarianceBiologicallyinspired_2007,
  title = {Box Invariance for Biologically-Inspired Dynamical Systems},
  booktitle = {2007 46th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Abate, Alessandro and Tiwari, Ashish and Sastry, Shankar},
  year = {2007},
  pages = {5162--5167},
  abstract = {In this paper, motivated by models drawn from biology, we introduce the notion of box invariant dynamical systems. We argue that box invariance, that is, the existence of a ``box''-shaped positively invariant region, is a characteristic of many biologically-inspired dynamical models. Box invariance is also useful for the verification of stability and safety properties of such systems. This paper presents effective characterization of this notion for some classes of systems, computational results on checking box invariance, the study of the dynamical properties it subsumes, and a comparison with related concepts in the literature. The concept is illustrated using models derived from different case studies in biology.},
  language = {en}
}
% == BibTeX quality report for abate_BoxInvarianceBiologicallyinspired_2007:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{amos_InputConvexNeural_2017,
  title = {Input {{Convex Neural Networks}}},
  author = {Amos, Brandon and Xu, Lei and Kolter, J. Zico},
  year = {2017},
  abstract = {This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made inputconvex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.},
  archivePrefix = {arXiv},
  journal = {arXiv:1609.07152 [cs, math]},
  language = {en},
  primaryClass = {cs, math}
}
% == BibTeX quality report for amos_InputConvexNeural_2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1609.07152 [cs, math]
% ? Title looks like it was stored in title-case in Zotero

@article{angeli_MonotoneControlSystems_2002,
  title = {Monotone Control Systems},
  author = {Angeli, David and Sontag, Eduardo D.},
  year = {2002},
  volume = {48},
  pages = {1684--1698},
  abstract = {Monotone systems constitute one of the most important classes of dynamical systems used in mathematical biology modeling. The objective of this paper is to extend the notion of monotonicity to systems with inputs and outputs, a necessary first step in trying to understand interconnections, especially including feedback loops, built up out of monotone components. Basic definitions and theorems are provided, as well as an application to the study of a model of one of the cell's most important subsystems.},
  journal = {IEEE Trans. Automat. Contr.}
}
% == BibTeX quality report for angeli_MonotoneControlSystems_2002:
% Missing required field 'number'
% ? Possibly abbreviated journal title IEEE Trans. Automat. Contr.

@article{archer_ApplicationBackPropagation_1993,
  title = {Application of the {{Back Propagation Neural Network Algorithm}} with {{Monotonicity Constraints}} for {{Two}}-{{Group Classification Problems}}},
  author = {Archer, Norman P. and Wang, Shouhong},
  year = {1993},
  volume = {24},
  pages = {60--75},
  abstract = {Neural network techniques are widely used in solving pattern recognition or classification problems. However, when statistical data are used in supervised training of a neural network employing the back-propagation least mean square algorithm, the behavior of the classification boundary during training is often unpredictable. This research suggests the application of monotonicity constraints to the back propagation learning algorithm. When the training sample set is preprocessed by a linear classification function, neural network performance and efficiency can be improved in classification applications where the feature vector is related monotonically to the pattern vector. Since most classification problems in business possess monotonic properties, this technique is useful in those problems where any assumptions about the properties of the data are inappropriate.},
  journal = {Decision Sciences},
  language = {en},
  number = {1}
}
% == BibTeX quality report for archer_ApplicationBackPropagation_1993:
% ? Title looks like it was stored in title-case in Zotero

@article{ben-david_MonotonicityMaintenanceInformationTheoretic_1995,
  title = {Monotonicity {{Maintenance}} in {{Information}}-{{Theoretic Machine Learning Algorithms}}},
  author = {{Ben-David}, Arie},
  year = {1995},
  volume = {19},
  pages = {29--43},
  abstract = {Decision trees that are based on information-theory are useful paradigms for learning from examples. However, in some real-world applications, known information-theoretic methods frequently generate nonmonotonic decision trees, in which objects with better attribute values are sometimes classified to lower classes than objects with inferior values. This property is undesirable for problem solving in many application domains, such as credit scoring and insurance premium determination, where monotonicity of subsequent classifications is important. An attribute-selection metric is proposed here that takes both the error as well as monotonicity into account while building decision trees. The metric is empirically shown capable of significantly reducing the degree of non-monotonicity of decision trees without sacrificing their inductive accuracy.},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}
% == BibTeX quality report for ben-david_MonotonicityMaintenanceInformationTheoretic_1995:
% ? Title looks like it was stored in title-case in Zotero

@incollection{chang_NeuralLyapunovControl_2019,
  title = {Neural {{Lyapunov Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Chang, Ya-Chien and Roohi, Nima and Gao, Sicun},
  year = {2019},
  pages = {3240--3249}
}
% == BibTeX quality report for chang_NeuralLyapunovControl_2019:
% Missing required field 'publisher'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{chen_TaylorModelFlowpipe_2012,
  title = {Taylor {{Model Flowpipe Construction}} for {{Non}}-Linear {{Hybrid Systems}}},
  booktitle = {2012 {{IEEE}} 33rd {{Real}}-{{Time Systems Symposium}}},
  author = {Chen, Xin and {\'A}brah{\'a}m, Erika and Sankaranarayanan, Sriram},
  year = {2012},
  pages = {183--192},
  abstract = {We propose an approach for verifying non-linear hybrid systems using higher-order Taylor models that are a combination of bounded degree polynomials over the initial conditions and time, bloated by an interval. Taylor models are an effective means for computing rigorous bounds on the complex time trajectories of non-linear differential equations. As a result, Taylor models have been successfully used to verify properties of non-linear continuous systems. However, the handling of discrete (controller) transitions remains a challenging problem. In this paper, we provide techniques for handling the effect of discrete transitions on Taylor model flow pipe construction. We explore various solutions based on two ideas: domain contraction and range over-approximation. Instead of explicitly computing the intersection of a Taylor model with a guard set, domain contraction makes the domain of a Taylor model smaller by cutting away parts for which the intersection is empty. It is complemented by range over-approximation that translates Taylor models into commonly used representations such as template polyhedra or zonotopes, on which intersections with guard sets have been previously studied. We provide an implementation of the techniques described in the paper and evaluate the various design choices over a set of challenging benchmarks.}
}
% == BibTeX quality report for chen_TaylorModelFlowpipe_2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{chilinski_NeuralLikelihoodsCumulative_2018,
  title = {Neural {{Likelihoods}} via {{Cumulative Distribution Functions}}},
  author = {Chilinski, Pawel and Silva, Ricardo},
  year = {2018},
  abstract = {We leverage neural networks as universal approximators of monotonic functions to build a parameterization of conditional cumulative distribution functions. By a modification of backpropagation as applied both to parameters and outputs, we show that we are able to build black box density estimators which are competitive against recently proposed models, while avoiding assumptions concerning the base distribution in a mixture model. That is, it makes no use of parametric models as building blocks. This approach removes some undesirable degrees of freedom on the design on neural networks for flexible conditional density estimation, while implementation can be easily accomplished by standard algorithms readily available in popular neural network toolboxes.},
  archivePrefix = {arXiv},
  journal = {arXiv:1811.00974 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}
% == BibTeX quality report for chilinski_NeuralLikelihoodsCumulative_2018:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1811.00974 [cs, stat]
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{coogan_EfficientFiniteAbstraction_2015,
  title = {Efficient Finite Abstraction of Mixed Monotone Systems},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Hybrid Systems Computation}} and {{Control}} - {{HSCC}} '15},
  author = {Coogan, Samuel and Arcak, Murat},
  year = {2015},
  pages = {58--67},
  abstract = {We present an efficient computational procedure for finite abstraction of discrete-time mixed monotone systems by considering a rectangular partition of the state space. Mixed monotone systems are decomposable into increasing and decreasing components, and significantly generalize the well known class of monotone systems. We tightly overapproximate the one-step reachable set from a box of initial conditions by computing a decomposition function at only two points, regardless of the dimension of the state space. We apply our results to verify the dynamical behavior of a model for insect population dynamics and to synthesize a signaling strategy for a traffic network.},
  language = {en}
}
% == BibTeX quality report for coogan_EfficientFiniteAbstraction_2015:
% Missing required field 'publisher'

@article{costantino_ExperimentallyInducedTransitions_1995,
  title = {Experimentally Induced Transitions in the Dynamic Behaviour of Insect Populations},
  author = {Costantino, R. F. and Cushing, J. M. and Dennis, Brian and Desharnais, Robert A.},
  year = {1995},
  volume = {375},
  pages = {227--230},
  abstract = {SIMPLE nonlinear models can generate fixed points, periodic cycles and aperiodic oscillations in population abundance without any external environmental variation. Another familiar theoretical result is that shifts in demographic parameters (such as survival or fecundity) can move a population from one of these behaviours to another1\textendash 4. Unfortunately, empirical evidence to support these theoretical possibilities is scarce5\textendash 15. We report here a joint theoretical and experimental study to test the hypothesis that changes in demographic parameters cause predictable changes in the nature of population fluctuations. Specifically, we developed a simple model describing population growth in the flour beetle Tribolium16. We then predicted, using standard mathematical techniques to analyse the model, that changes in adult mortality would produce substantial shifts in population dynamic behaviour. Finally, by experimentally manipulating the adult mortality rate we observed changes in the dynamics from stable fixed points to periodic cycles to aperiodic oscillations that corresponded to the transitions forecast by the mathematical model.},
  copyright = {1995 Nature Publishing Group},
  journal = {Nature},
  language = {en},
  number = {6528}
}

@inproceedings{dang_AccurateHybridizationNonlinear_2010,
  title = {Accurate Hybridization of Nonlinear Systems},
  booktitle = {Proceedings of the 13th {{ACM}} International Conference on {{Hybrid}} Systems: Computation and Control - {{HSCC}} '10},
  author = {Dang, Thao and Maler, Oded and Testylier, Romain},
  year = {2010},
  pages = {11},
  abstract = {This paper is concerned with reachable set computation for non-linear systems using hybridization. The essence of hybridization is to approximate a non-linear vector field by a simpler (such as affine) vector field. This is done by partitioning the state space into small regions within each of which a simpler vector field is defined. This approach relies on the availability of methods for function approximation and for handling the resulting dynamical systems. Concerning function approximation using interpolation, the accuracy depends on the shapes and sizes of the regions which can compromise as well the speed of reachability computation since it may generate spurious classes of trajectories. In this paper we study the relationship between the region geometry and reachable set accuracy and propose a method for constructing hybridization regions using tighter interpolation error bounds. In addition, our construction exploits the dynamics of the system to adapt the orientation of the regions, in order to achieve better time-efficiency. We also present some experimental results on a high-dimensional biological system, to demonstrate the performance improvement.},
  language = {en}
}
% == BibTeX quality report for dang_AccurateHybridizationNonlinear_2010:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{daniels_MonotonePartiallyMonotone_2010,
  title = {Monotone and {{Partially Monotone Neural Networks}}},
  author = {Daniels, Hennie and Velikova, Marina},
  year = {2010},
  volume = {21},
  pages = {906--917},
  abstract = {In many classification and prediction problems it is known that the response variable depends on certain explanatory variables. Monotone neural networks can be used as powerful tools to build monotone models with better accuracy and lower variance compared to ordinary nonmonotone models. Monotonicity is usually obtained by putting constraints on the parameters of the network. In this paper, we will clarify some of the theoretical results on monotone neural networks with positive weights, issues that are sometimes misunderstood in the neural network literature. Furthermore, we will generalize some of the results obtained by Sill for the so-called MIN\textendash MAX networks to the case of partially monotone problems. The method is illustrated in practical case studies.},
  journal = {IEEE Transactions on Neural Networks},
  language = {en},
  number = {6}
}
% == BibTeX quality report for daniels_MonotonePartiallyMonotone_2010:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{fard_FastFlexibleMonotonic_2016,
  title = {Fast and {{Flexible Monotonic Functions}} with {{Ensembles}} of {{Lattices}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fard, Mahdi Milani and Canini, Kevin and Cotter, Andrew and Pfeifer, Jan and Gupta, Maya},
  year = {2016},
  pages = {2919--2927},
  abstract = {For many machine learning problems, there are some inputs that are known to be positively (or negatively) related to the output, and in such cases training the model to respect that monotonic relationship can provide regularization, and makes the model more interpretable. However, flexible monotonic functions are computationally challenging to learn beyond a few features. We break through this barrier by learning ensembles of monotonic calibrated interpolated look-up tables (lattices). A key contribution is an automated algorithm for selecting feature subsets for the ensemble base models. We demonstrate that compared to random forests, these ensembles produce similar or better accuracy, while providing guaranteed monotonicity consistent with prior knowledge, smaller model size and faster evaluation.},
  language = {en}
}
% == BibTeX quality report for fard_FastFlexibleMonotonic_2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{garcia_MoNGELMonotonicNested_2017,
  title = {{{MoNGEL}}: Monotonic Nested Generalized Exemplar Learning},
  author = {Garc{\'i}a, Javier and Fardoun, Habib M. and Alghazzawi, Daniyal M. and Cano, Jos{\'e}-Ram{\'o}n and Garc{\'i}a, Salvador},
  year = {2017},
  volume = {20},
  pages = {441--452},
  journal = {Pattern Analysis and Applications},
  language = {en},
  number = {2}
}

@article{gupta_HowIncorporateMonotonicity_2019,
  title = {How to {{Incorporate Monotonicity}} in {{Deep Networks While Preserving Flexibility}}?},
  author = {Gupta, Akhil and Shukla, Naman and Marla, Lavanya and Kolbeinsson, Arinbj{\"o}rn and Yellepeddi, Kartik},
  year = {2019},
  abstract = {The importance of domain knowledge in enhancing model performance and making reliable predictions in the real-world is critical. This has led to an increased focus on specific model properties for interpretability. We focus on incorporating monotonic trends, and propose a novel gradient-based point-wise loss function for enforcing partial monotonicity with deep neural networks. While recent developments have relied on structural changes to the model, our approach aims at enhancing the learning process. Our model-agnostic point-wise loss function acts as a plug-in to the standard loss and penalizes non-monotonic gradients. We demonstrate that the point-wise loss produces comparable (and sometimes better) results on both AUC and monotonicity measure, as opposed to state-of-the-art deep lattice networks that guarantee monotonicity. Moreover, it is able to learn differentiated individual trends and produces smoother conditional curves which are important for personalized decisions, while preserving the flexibility of deep networks.},
  archivePrefix = {arXiv},
  journal = {arXiv:1909.10662 [cs]},
  language = {en},
  primaryClass = {cs}
}
% == BibTeX quality report for gupta_HowIncorporateMonotonicity_2019:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1909.10662 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{gupta_MonotonicCalibratedInterpolated_2016,
  title = {Monotonic {{Calibrated Interpolated Look}}-{{Up Tables}}},
  author = {Gupta, Maya and Cotter, Andrew and Pfeifer, Jan and Voevodski, Konstantin and Canini, Kevin and Mangylov, Alexander and Moczydlowski, Wojciech and {van Esbroeck}, Alexander},
  year = {2016},
  pages = {47},
  abstract = {Real-world machine learning applications may have requirements beyond accuracy, such as fast evaluation times and interpretability. In particular, guaranteed monotonicity of the learned function with respect to some of the inputs can be critical for user confidence. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to monotonic functions by adding linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and random sampling of additive regularizer terms. Case studies on real-world problems with up to sixteen features and up to hundreds of millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy in practice while providing greater transparency to users.},
  language = {en}
}
% == BibTeX quality report for gupta_MonotonicCalibratedInterpolated_2016:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@article{hirsch_MonotoneDynamicalSystems_2006,
  title = {Monotone {{Dynamical Systems}}},
  author = {Hirsch, M W and Smith, Hal},
  year = {2006},
  volume = {2},
  pages = {239--357},
  journal = {Handbook of differential equations: ordinary differential equations},
  language = {en}
}
% == BibTeX quality report for hirsch_MonotoneDynamicalSystems_2006:
% Missing required field 'number'
% ? Title looks like it was stored in title-case in Zotero

@article{hirsch_MonotoneMapsReview_2005,
  title = {Monotone Maps: A Review},
  author = {Hirsch, M. W. and Smith, Hal},
  year = {2005},
  volume = {11},
  pages = {379--398},
  abstract = {The aim of this paper is to provide a brief review of the main results in the theory of discrete-time monotone dynamics.},
  journal = {Journal of Difference Equations and Applications},
  language = {en},
  number = {4-5}
}

@article{hu_DesignGeneralProjection_2007,
  title = {Design of {{General Projection Neural Networks}} for {{Solving Monotone Linear Variational Inequalities}} and {{Linear}} and {{Quadratic Optimization Problems}}},
  author = {Hu, Xiaolin and Wang, Jun},
  year = {2007},
  volume = {37},
  pages = {1414--1421},
  abstract = {Most existing neural networks for solving linear variational inequalities (LVIs) with the mapping Mx + p require positive definiteness (or positive semidefiniteness) of M. In this correspondence, it is revealed that this condition is sufficient but not necessary for an LVI being strictly monotone (or monotone) on its constrained set where equality constraints are present. Then, it is proposed to reformulate monotone LVIs with equality constraints into LVIs with inequality constraints only, which are then possible to be solved by using some existing neural networks. General projection neural networks are designed in this correspondence for solving the transformed LVIs. Compared with existing neural networks, the designed neural networks feature lower model complexity. Moreover, the neural networks are guaranteed to be globally convergent to solutions of the LVI under the condition that the linear mapping Mx + p is monotone on the constrained set. Because quadratic and linear programming problems are special cases of LVI in terms of solutions, the designed neural networks can solve them efficiently as well. In addition, it is discovered that the designed neural network in a specific case turns out to be the primal-dual network for solving quadratic or linear programming problems. The effectiveness of the neural networks is illustrated by several numerical examples.},
  journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
  language = {en},
  number = {5}
}
% == BibTeX quality report for hu_DesignGeneralProjection_2007:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{kolter_LearningStableDeep_2019,
  title = {Learning {{Stable Deep Dynamics Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kolter, J Zico and Manek, Gaurav},
  year = {2019},
  pages = {11126--11134},
  abstract = {Deep networks are commonly used to model dynamical systems, predicting how the state of a system will evolve over time (either autonomously or in response to control inputs). Despite the predictive power of these systems, it has been difficult to make formal claims about the basic properties of the learned systems. In this paper, we propose an approach for learning dynamical systems that are guaranteed to be stable over the entire state space. The approach works by jointly learning a dynamics model and Lyapunov function that guarantees non-expansiveness of the dynamics under the learned Lyapunov function. We show that such learning systems are able to model simple dynamical systems and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion.},
  language = {en}
}
% == BibTeX quality report for kolter_LearningStableDeep_2019:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{leenheer_MonotoneChemicalReaction_2007,
  title = {Monotone {{Chemical Reaction Networks}}},
  author = {Leenheer, Patrick De and Angeli, David and Sontag, Eduardo D.},
  year = {2007},
  volume = {41},
  pages = {295--314},
  abstract = {We analyze certain chemical reaction networks and show that every solution converges to some steady state. The reaction kinetics are assumed to be monotone but otherwise arbitrary. When diffusion effects are taken into account, the conclusions remain unchanged. The main tools used in our analysis come from the theory of monotone dynamical systems. We review some of the features of this theory and provide a self-contained proof of a particular attractivity result which is used in proving our main result.},
  journal = {Journal of Mathematical Chemistry},
  language = {en},
  number = {3}
}
% == BibTeX quality report for leenheer_MonotoneChemicalReaction_2007:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{liang_WhyDeepNeural_2017,
  title = {Why {{Deep Neural Networks}} for {{Function Approximation}}?},
  booktitle = {The 5th {{International Conference}} on {{Learning Representations}}},
  author = {Liang, Shiyu and Srikant, R.},
  year = {2017},
  abstract = {Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of {$\epsilon$} uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on {$\epsilon$}) require {$\Omega$}(poly(1/{$\epsilon$})) neurons while deep networks (i.e., networks whose depth grows with 1/{$\epsilon$}) require O(polylog(1/{$\epsilon$})) neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.},
  archivePrefix = {arXiv},
  language = {en}
}
% == BibTeX quality report for liang_WhyDeepNeural_2017:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{mukarjee_FeasibleNonparametricEstimation_1994,
  title = {Feasible {{Nonparametric Estimation}} of {{Multiargument Monotone Functions}}},
  author = {Mukarjee, Hari and Stern, Steven},
  year = {1994},
  volume = {89},
  pages = {77--80},
  abstract = {This article presents a two-stage estimation procedure that uses an ad hoc but very easily implemented isotonization of a kernel estimator. This procedure yields an isotonic estimator with the convergence properties of the kernel estimator. Although the isotonization in the second stage does not satisfy the least squares condition, this hybrid estimator may be considered to be a multidimensional generalization of similar procedures for the one-dimensional case suggested by Friedman and Tibshirani and by Mukarjee. We derive some of the asymptotic properties of our estimator and demonstrate other statistical properties with Monte Carlo studies. We conclude by providing a real data example.},
  journal = {Journal of the American Statistical Association},
  number = {425}
}
% == BibTeX quality report for mukarjee_FeasibleNonparametricEstimation_1994:
% ? Title looks like it was stored in title-case in Zotero

@article{neumann_ReliableIntegrationContinuous_2013,
  title = {Reliable Integration of Continuous Constraints into Extreme Learning Machines},
  author = {Neumann, Klaus and Rolf, Matthias and Steil, Jochen J.},
  year = {2013},
  volume = {21},
  pages = {35--50},
  abstract = {The application of machine learning methods in the engineering of intelligent technical systems often requires the integration of continuous constraints like positivity, monotonicity, or bounded curvature in the learned function to guarantee a reliable performance. We show that the extreme learning machine is particularly well suited for this task. Constraints involving arbitrary derivatives of the learned function are effectively implemented through quadratic optimization because the learned function is linear in its parameters, and derivatives can be derived analytically. We further provide a constructive approach to verify that discretely sampled constraints are generalized to continuous regions and show how local violations of the constraint can be rectified by iterative re-learning. We demonstrate the approach on a practical and challenging control problem from robotics, illustrating also how the proposed method enables learning from few data samples if additional prior knowledge about the problem is...},
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge Based Systems},
  number = {supp02}
}

@inproceedings{pan_LearningDeepNeural_2017,
  title = {Learning {{Deep Neural Network Control Policies}} for {{Agile Off}}-{{Road Autonomous Driving}}},
  booktitle = {The {{NIPS Deep Reinforcement Learning Symposium}}},
  author = {Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
  year = {2017},
  pages = {13},
  abstract = {We present an end-to-end learning system for agile, off-road autonomous driving using only low-cost on-board sensors. By imitating an optimal controller, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands, the latter of which is essential to successfully drive on varied terrain at high speed. Compared with recent approaches to similar tasks, our method requires neither state estimation nor online planning to navigate the vehicle. Real-world experimental results demonstrate successful autonomous driving, matching the state-of-the-art performance.},
  language = {en}
}
% == BibTeX quality report for pan_LearningDeepNeural_2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{potharst_ClassificationTreesProblems_2002,
  title = {Classification Trees for Problems with Monotonicity Constraints},
  author = {Potharst, R. and Feelders, A. J.},
  year = {2002},
  volume = {4},
  pages = {1},
  journal = {ACM SIGKDD Explorations Newsletter},
  language = {en},
  number = {1}
}

@inproceedings{richards_LyapunovNeuralNetwork_2018,
  title = {The {{Lyapunov Neural Network}}: {{Adaptive Stability Certification}} for {{Safe Learning}} of {{Dynamical Systems}}},
  booktitle = {Proceedings of {{The}} 2nd {{Conference}} on {{Robot Learning}}},
  author = {Richards, Spencer M. and Berkenkamp, Felix and Krause, Andreas},
  year = {2018},
  pages = {466--476},
  abstract = {Learning algorithms have shown considerable prowess in simulation by allowing robots to adapt to uncertain environments and improve their performance. However, such algorithms are rarely used in practice on safety-critical systems, since the learned policy typically does not yield any safety guarantees. That is, the required exploration may cause physical harm to the robot or its environment. In this paper, we present a method to learn accurate safety certificates for nonlinear, closed-loop dynamical systems. Specifically, we construct a neural network Lyapunov function and a training algorithm that adapts it to the shape of the largest safe region in the state space. The algorithm relies only on knowledge of inputs and outputs of the dynamics, rather than on any specific model structure. We demonstrate our method by learning the safe region of attraction for a simulated inverted pendulum. Furthermore, we discuss how our method can be used in safe learning algorithms together with statistical models of dynamical systems.},
  archivePrefix = {arXiv}
}
% == BibTeX quality report for richards_LyapunovNeuralNetwork_2018:
% Missing required field 'publisher'
% ? Title looks like it was stored in title-case in Zotero

@incollection{sill_MonotonicityHints_1997,
  title = {Monotonicity {{Hints}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 9},
  author = {Sill, Joseph and {Abu-Mostafa}, Yaser S.},
  year = {1997},
  pages = {634--640}
}
% == BibTeX quality report for sill_MonotonicityHints_1997:
% Missing required field 'publisher'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{sill_MonotonicNetworks_1998,
  title = {Monotonic {{Networks}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sill, Joseph},
  year = {1998},
  pages = {661--667},
  abstract = {Monotonicity is a constraint which arises in many application domains. We present a machine learning model, the monotonic network, for which monotonicity can be enforced exactly, i.e., by virtue offunctional form . A straightforward method for implementing and training a monotonic network is described. Monotonic networks are proven to be universal approximators of continuous, differentiable monotonic functions. We apply monotonic networks to a real-world task in corporate bond rating prediction and compare them to other approaches.},
  language = {en}
}
% == BibTeX quality report for sill_MonotonicNetworks_1998:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@book{smith_MonotoneDynamicalSystems_2008,
  title = {Monotone {{Dynamical Systems}}: {{An Introduction}} to the {{Theory}} of {{Competitive}} and {{Cooperative Systems}}},
  author = {Smith, Hal},
  year = {2008},
  volume = {41},
  abstract = {Advancing research. Creating connections.},
  language = {en}
}
% == BibTeX quality report for smith_MonotoneDynamicalSystems_2008:
% Missing required field 'publisher'
% ? Title looks like it was stored in title-case in Zotero

@article{srivastava_DropoutSimpleWay_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  volume = {15},
  pages = {1929--1958},
  journal = {Journal of Machine Learning Research}
}
% == BibTeX quality report for srivastava_DropoutSimpleWay_2014:
% Missing required field 'number'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{wehenkel_UnconstrainedMonotonicNeural_2019,
  title = {Unconstrained {{Monotonic Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wehenkel, Antoine and Louppe, Gilles},
  year = {2019},
  pages = {1543--1553},
  abstract = {Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations. In this work, we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output. We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its effectiveness on density estimation experiments. We also illustrate the ability of UMNNs to improve variational inference.},
  archivePrefix = {arXiv},
  language = {en}
}
% == BibTeX quality report for wehenkel_UnconstrainedMonotonicNeural_2019:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{xia_GeneralProjectionNeural_2004,
  title = {A {{General Projection Neural Network}} for {{Solving Monotone Variational Inequalities}} and {{Related Optimization Problems}}},
  author = {Xia, Y. and Wang, J.},
  year = {2004},
  volume = {15},
  pages = {318--328},
  abstract = {Recently, a projection neural network for solving monotone variational inequalities and constrained optimization problems was developed. In this paper, we propose a general projection neural network for solving a wider class of variational inequalities and related optimization problems. In addition to its simple structure and low complexity, the proposed neural network includes existing neural networks for optimization, such as the projection neural network, the primal-dual neural network, and the dual neural network, as special cases. Under various mild conditions, the proposed general projection neural network is shown to be globally convergent, globally asymptotically stable, and globally exponentially stable. Furthermore, several improved stability criteria on two special cases of the general projection neural network are obtained under weaker conditions. Simulation results demonstrate the effectiveness and characteristics of the proposed neural network.},
  journal = {IEEE Transactions on Neural Networks},
  language = {en},
  number = {2}
}
% == BibTeX quality report for xia_GeneralProjectionNeural_2004:
% ? Title looks like it was stored in title-case in Zotero


